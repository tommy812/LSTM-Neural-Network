# -*- coding: utf-8 -*-
"""LSTM_Forex.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1etKiZC-aMK2tduHh6IY3JkJLZOexbZOW
"""

#only on first run 
!pip install pandas_ta

import numpy as np
import pandas as pd
import yfinance as yf
import pandas_ta as ta
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from keras.models import Model
from keras.layers import Dense, Dropout, LSTM, Input, Activation
from keras import optimizers

# Define the currency pair and time range
#currency_pair = "EURUSD=X"
currency_pair = "USDJPY=X"
start_date = "2010-01-01"
end_date = "2021-12-31"


# Download the historical data using YFinance API
data = yf.download(currency_pair, start=start_date, end=end_date)

 
# Remove unecessary columns from the dataset
data.reset_index(inplace = True)
data = data.drop(['Volume','Adj Close'], axis=1)
pd.set_option('display.max_columns', None)
 
# Create Custom Strategy
CustomStrategy = ta.Strategy(
    name="Momo, Bands and SMAs and Cumulative Log Returns",
    description="MACD and RSI Momo with BBANDS and SMAs 50 & 200 ",
    ta=[
        {"kind":"sma", "length": 50},
      {"kind":"sma", "length": 200},
      {"kind":"bbands", "length": 20, "ddof": 0},
      {"kind":"macd"},
      {"kind":"rsi"},     
    ]
)
# To run your "Custom Strategy"
data.ta.strategy(CustomStrategy)
 #set Target to the difference between the current cl price and tommorrow cl price
data['Target'] = data['Close']-data.Open
data['Target'] = data['Target'].shift(-1)
#set target next Close to the close value of next day
data['TargetNextClose'] = data['Close'].shift(-1)

# Drop missing values
data.dropna(inplace=True)
# Initialize the scaler
scaler = MinMaxScaler(feature_range=(0,1))

# Scale the data (excluding the 'Date' column)
scaled_data = scaler.fit_transform(data.iloc[:, 1:])
#save the dates
dates =  data.iloc[:, 0]







# Define the parameter space
epochs_list = [25,50,100]
back_candles_list = [5,15,30]
batch_sizes = [10,15,20]

# Define the Mean Absolute Percentage Error function
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

# Train and evaluate the model for a given set of parameters
def train_evaluate_model(epochs, back_candles, batch_size, X_train, y_train, X_test, y_test, scaler):
    print("epochs: "+str(epochs)+", BackCandles: "+ str(back_candles)+", Batch Size: "+str(batch_size))


    # Split the data into train and test sets
    X=[] # input data for the model

    #number of the columns to feed the model
    nColumns = 15



    #get the only the columns that need to be fed to the model abd store in X
    for j in range(nColumns):
      X.append([])
      for i in range(back_candles, scaled_data.shape[0]):
        X[j].append(scaled_data[i-back_candles:i, j])


    #move axis from 0 to position 2
    X=np.moveaxis(X, [0], [2])

    #set prediction to the targetnext Column: -1
    #set prediction to the target Column: -2

    prediction = -1
    #prediction = -2

    X, yi =np.array(X), np.array(scaled_data[back_candles:,prediction])
    y=np.reshape(yi,(len(yi),1))

    # split data into train test sets
    splitlimit = int(len(X)*0.8)

    X_train, X_test = X[:splitlimit], X[splitlimit:]
    y_train, y_test = y[:splitlimit], y[splitlimit:]

    # Build the model
    lstm_input = Input(shape=(back_candles, nColumns), name='lstm_input')
    hidden_nodes = int(2/3 * (back_candles * nColumns))

    inputs = LSTM(hidden_nodes, name='first_layer')(lstm_input)
    inputs = Dense(1, name='dense_layer', activation='relu')(inputs)
    output = Activation('linear', name='output')(inputs)

    model = Model(inputs=lstm_input, outputs=output)

    adam = optimizers.Adam(learning_rate=0.001)
    model.compile(loss='mse', optimizer=adam, metrics=['mean_absolute_error'])

    # Train the model
    model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_split=0.1)

    # Evaluate the model on the test set and compute the evaluation metrics
    y_pred = model.predict(X_test)
    # Create dummy arrays with the same shape as the original dataset
    dummy_test = np.zeros((len(y_test), scaled_data.shape[1]))
    dummy_pred = np.zeros((len(y_pred), scaled_data.shape[1]))
    # target column is the last column in the dataset
    target_col = -1
    # Replace the target column with the scaled y_test and y_pred values
    dummy_test[:, target_col] = y_test[:, 0]
    dummy_pred[:, target_col] = y_pred[:, 0]
    # Descale the target values
    y_test_inverse = scaler.inverse_transform(dummy_test)[:, target_col]
    y_pred_inverse = scaler.inverse_transform(dummy_pred)[:, target_col]
    # Calculate the error metrics
    mae = mean_absolute_error(y_test_inverse, y_pred_inverse)
    mse = mean_squared_error(y_test_inverse, y_pred_inverse)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_test_inverse, y_pred_inverse)
  
    print("Mean Absolute Error:", mae, "Mean Squared Error:", mse,"Root Mean Squared Error:", rmse,"Mean Absolute Percentage Error:", mape)

  
    #plot everything 
    plt.figure(figsize=(nColumns, 8))
    plt.plot(y_test_inverse, color='black', label='Test')
    plt.plot(y_pred_inverse, color='green', label='pred')
    plt.legend()
    plt.show()

    return mae, mse, rmse, mape, X_train, y_train, X_test, y_test

# Loop through all combinations of parameter values and store the evaluation metrics
results = []
X_train = []
y_train = []
X_test = []
y_test = []

for epochs in epochs_list:
    for back_candles in back_candles_list:
        for batch_size in batch_sizes:
            mae, mse, rmse, mape, X_train, y_train, X_test, y_test = train_evaluate_model(epochs, back_candles, batch_size, X_train, y_train, X_test, y_test, scaler)
            results.append({
                "epochs": epochs,
                "back_candles": back_candles,
                "batch_size": batch_size,
                "mae": mae,
                "mse": mse,
                "rmse": rmse,
                "mape": mape
        })
            
# Convert the results list to a DataFrame for easier analysis
results_df = pd.DataFrame(results)
results_df.sort_values(by="mae", inplace=True)

print("Top models sorted by Mean Absolute Error:")
print(results_df.head(10))