# -*- coding: utf-8 -*-
"""FinalDesign.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XwdlgWLq0EV6W6pcFqgseGWfR3LR80Nm
"""

!pip install pandas_ta

import yfinance as yf
import pandas as pd
# Define the currency pair and time range
currency_pair = "USDJPY=X"
start_date = "2010-01-01"
end_date = "2021-12-31"


# Download the historical data using YFinance API
data = yf.download(currency_pair, start=start_date, end=end_date)

 
# Remove unecessary columns from the dataset
data.reset_index(inplace = True)
data = data.drop(['Volume','Adj Close'], axis=1)
pd.set_option('display.max_columns', None)
 

# Display dataset
data.head()



import pandas_ta as ta

# Create Custom Strategy
CustomStrategy = ta.Strategy(
    name="Momo, Bands and SMAs and Cumulative Log Returns",
    description="MACD and RSI Momo with BBANDS and SMAs 50 & 200 ",
    ta=[
        {"kind":"sma", "length": 50},
      {"kind":"sma", "length": 200},
      {"kind":"bbands", "length": 20, "ddof": 0},
      {"kind":"macd"},
      {"kind":"rsi"},     
    ]
)
# To run your "Custom Strategy"
data.ta.strategy(CustomStrategy)
 #set Target to the difference between the current cl price and tommorrow cl price
data['Target'] = data['Close']-data.Open
data['Target'] = data['Target'].shift(-1)
#set target next Close to the close value of next day
data['TargetNextClose'] = data['Close'].shift(-1)

# Drop missing values
data.dropna(inplace=True)

print(data)

from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler(feature_range=(0,1))

# Scale the data (excluding the 'Date' column)
scaled_data = scaler.fit_transform(data.iloc[:, 1:])
#save the dates
dates =  data.iloc[:, 0]

print(scaled_data)

import numpy as np
X=[] # input data for the model


#Number of days to be processed
backcandles = 15;
#number of the columns to feed the model
nColumns = 15

print(scaled_data.shape)

#get the only the columns that need to be fed to the model abd store in X
for j in range(nColumns):
  X.append([])
  for i in range(backcandles, scaled_data.shape[0]):
    X[j].append(scaled_data[i-backcandles:i, j])


#move axis from 0 to position 2
X=np.moveaxis(X, [0], [2])

#set prediction to the targetnext Column: -1
#set prediction to the target Column: -2

prediction = -1
#prediction = -2

X, yi =np.array(X), np.array(scaled_data[backcandles:,prediction])
y=np.reshape(yi,(len(yi),1))

#verify the shape of the data
print(X.shape)
print(y.shape)

# split data into train test sets
splitlimit = int(len(X)*0.8)
print(splitlimit)
X_train, X_test = X[:splitlimit], X[splitlimit:]
y_train, y_test = y[:splitlimit], y[splitlimit:]
#same number of column but less entries in test
print(X_train.shape)
print(X_test.shape)
#same number of column but less entries in test
print(y_train.shape)
print(y_test.shape)

from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Dense
from keras.layers import TimeDistributed

import tensorflow as tf
import keras
from keras import optimizers
from keras.callbacks import History
from keras.models import Model
from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate

import numpy as np
#tf.random.set_seed(20)
np.random.seed(10)


#model 1
#shape of the feeding data
lstm_input = Input(shape=(backcandles, nColumns), name='lstm_input')

#n hidden layers
hidden_nodes = int(2/3 * (backcandles * nColumns))
print(hidden_nodes)

#layers 
inputs = LSTM(hidden_nodes, name='first_layer')(lstm_input)# sigmoind
inputs = Dense(1, name='dense_layer', activation='relu')(inputs)
#inputs = Dense(32, name='dense_layer2', activation='relu')(inputs)
output = Activation('linear', name='output')(inputs)
model = Model(inputs=lstm_input, outputs=output)
 

adam = optimizers.Adam(learning_rate=0.001)

#compiling
model.compile(loss='mse',optimizer=adam, metrics=['mean_absolute_error'])



model.fit(x=X_train, y=y_train, batch_size=10, epochs=100, shuffle=True, validation_split = 0.1)

from sklearn.metrics import mean_absolute_error, accuracy_score, mean_squared_error
# Define the Mean Absolute Percentage Error function
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

#apply the model to the test data 
y_pred = model.predict(X_test)

print(y_pred.shape)
print(y_test.shape)


# Create dummy arrays with the same shape as the original dataset
dummy_test = np.zeros((len(y_test), scaled_data.shape[1]))
dummy_pred = np.zeros((len(y_pred), scaled_data.shape[1]))

# target column is the last column in the dataset
target_col = -1

# Replace the target column with the scaled y_test and y_pred values
dummy_test[:, target_col] = y_test[:, 0]
dummy_pred[:, target_col] = y_pred[:, 0]

# Descale the target values
y_test_inverse = scaler.inverse_transform(dummy_test)[:, target_col]
y_pred_inverse = scaler.inverse_transform(dummy_pred)[:, target_col]


# Calculate the error metrics
mae = mean_absolute_error(y_test_inverse, y_pred_inverse)
print("Mean Absolute Error:", mae)

mse = mean_squared_error(y_test_inverse, y_pred_inverse)
print("Mean Squared Error:", mse)

rmse = np.sqrt(mse)
print("Root Mean Squared Error:", rmse)

mape = mean_absolute_percentage_error(y_test_inverse, y_pred_inverse)
print("Mean Absolute Percentage Error:", mape)

# Print 10 predictions against actual values
for i in range(10):
    print(y_pred_inverse[i], y_test_inverse[i])

import matplotlib.pyplot as plt

#plot everything 
plt.figure(figsize=(nColumns, 8))
plt.plot(y_test_inverse, color='black', label='Test')
plt.plot(y_pred_inverse, color='green', label='pred')
plt.legend()
plt.show()